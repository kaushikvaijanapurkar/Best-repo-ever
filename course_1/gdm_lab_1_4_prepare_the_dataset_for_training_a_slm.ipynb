{
  "cells": [
    {
      "metadata": {
        "id": "6kkanWKTSzW4"
      },
      "cell_type": "markdown",
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C1-white-bg.png\">"
      ],
      "metadata": {
        "id": "3myzQnLMOJ91"
      }
    },
    {
      "metadata": {
        "id": "keR5bAwlrIo4"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab: Prepare The Dataset For Training an SLM\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_4_prepare_the_dataset_for_training_a_slm.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Prepare the dataset so that a transformer model can process it.\n",
        "\n",
        "30 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "This lab guides you through preparing a text dataset for training a small language model (SLM). The SLM that you will train is a transformer model. Transformer models and, more generally, neural network models, require data to be in a specific format so that they can process them. Specifically, when processing texts, you first have to tokenize the text into tokens. Then, you have to translate these tokens into unique numeric IDs before you can process them with a transformer model.\n",
        "\n",
        "In this lab, you will focus on these necessary **pre-processing steps** such as tokenization, vocabulary creation, and mapping tokens to their IDs. This will build the foundation for training your SLM in the next lab.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WbdzNNC7Jrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will know:\n",
        "* The data format requirements of transformer models and how to map tokens to token IDs and vice versa.\n",
        "* How to prepare a dataset for training a transformer model."
      ],
      "metadata": {
        "id": "XYhCTN9FNLwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "As in previous labs, you will load the Africa Galore dataset and tokenize it using a space tokenizer. You will then build all the ingredients necessary for converting a dataset such that it can be used for training a transformer model.\n",
        "\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load the dataset and tokenize it.\n",
        "* Construct a list of all tokens in the dataset.\n",
        "* Construct a list of unique tokens in the dataset.\n",
        "* Create a mapping of tokens to token IDs and a mapping of token IDs to tokens.\n",
        "* Define functions that can translate between tokens and their corresponding IDs.\n",
        "* Define a Python class that encapsulates all methods necessary for preparing the data for a transformer model.\n"
      ],
      "metadata": {
        "id": "iefhLcRUNGPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ],
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are excuted on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (▶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or ⌘+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ],
      "metadata": {
        "id": "wlNG_jg-39Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ],
      "metadata": {
        "id": "UyTT6C0JhGBs",
        "outputId": "2cbf7966-b6c7-4cde-f316-81ebc89c232b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today is Saturday.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order. Otherwise, the code might not work. If you take a break while working on a lab, Colab may disconnect you. In that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime → Run before__  from the menu above (or use the keyboard combination Ctrl/⌘ + F8). This will re-execute all cells before the current one."
      ],
      "metadata": {
        "id": "pbtgZxrpjm6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "WQQlDe0hL8AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will use the [Pandas](http://pandas.pydata.org) package for reading the dataset. Run the following cell to import these packages."
      ],
      "metadata": {
        "id": "UPJE5CKOA2bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import re # Used for splitting strings on spaces.\n",
        "\n",
        "# Packages used.\n",
        "import pandas as pd # For reading the dataset.\n",
        "import textwrap # For adding linebreaks to paragraphs.\n",
        "\n",
        "# For providing feedback.\n",
        "from ai_foundations.feedback.course_1 import slm"
      ],
      "metadata": {
        "id": "m8QsDdu20F0C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aS8lTu4FDclK"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading and tokenizing the dataset\n",
        "\n",
        "As in the previous labs, you will again use the [Africa Galore](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json) dataset for the activities in this lab.\n",
        "\n",
        "Run the following cell to download the dataset and print its first paragraph."
      ]
    },
    {
      "metadata": {
        "id": "x8rRtd8p2DQ6",
        "outputId": "bd3c319b-6fd6-4d48-a1e2-8563258aba29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "dataset = africa_galore[\"description\"]\n",
        "print(f\"Loaded Africa Galore dataset with {len(dataset)} paragraphs.\")\n",
        "print(f\"\\nFirst paragraph:\")\n",
        "print(textwrap.fill(dataset[0]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Africa Galore dataset with 232 paragraphs.\n",
            "\n",
            "First paragraph:\n",
            "The Lagos air was thick with humidity, but the energy in the club was\n",
            "electric. The band launched into a hypnotic Afrobeat groove, the drums\n",
            "pounding out a complex polyrhythm, the horns blaring a soaring melody,\n",
            "and the bass laying down a deep, funky foundation. A woman named Imani\n",
            "moved effortlessly to the music, her body swaying in time with the\n",
            "rhythm. The music seemed to flow through her, a powerful current of\n",
            "energy and joy. All around her, people were dancing, singing, and\n",
            "clapping, caught up in the infectious rhythm. The music was more than\n",
            "just entertainment; it was a celebration of life, a connection to\n",
            "their shared heritage, a vibrant expression of the soul of Lagos.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with n-gram language models, you also have to tokenize sequences before you can use them to train a transformer model. You will again use a space tokenizer that splits sequences on spaces.\n",
        "\n",
        "Run the following cell to define and test the space tokenizer that is implemented by the function `space_tokenize`. This function is almost identical to the function you have already seen. Instead of the string `split` function, it uses the [`re.split`](https://docs.python.org/3/library/re.html#re.split) function, since it's better at handling texts that contain multiple spaces."
      ],
      "metadata": {
        "id": "W93OAFTde5Bg"
      }
    },
    {
      "metadata": {
        "id": "JjaimYtAx4j9",
        "outputId": "71541edd-4f4a-49e5-83a4-c25586a9e571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "def space_tokenize(text: str) -> list[str]:\n",
        "    \"\"\"Splits a string into a list of tokens.\n",
        "\n",
        "    Splits text on space.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        A list of tokens. Returns empty list if text is empty or all spaces.\n",
        "    \"\"\"\n",
        "    # Use `re` package so that splitting on multiple spaces also works.\n",
        "    tokens = re.split(r\" +\", text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Tokenize an example text with the `space_tokenize` function.\n",
        "space_tokenize(\"Kanga, a colorful printed cloth is more than just a fabric.\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Kanga,',\n",
              " 'a',\n",
              " 'colorful',\n",
              " 'printed',\n",
              " 'cloth',\n",
              " 'is',\n",
              " 'more',\n",
              " 'than',\n",
              " 'just',\n",
              " 'a',\n",
              " 'fabric.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1: Build a list of all tokens in the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "NnnZKo5MgcO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> 💻 **Your task:**\n",
        ">\n",
        "> Complete the following cell to construct a `tokens` list that contains all tokens in the dataset in the order that they appear in.\n",
        ">\n",
        "> You will have to loop through all paragraphs in the dataset and then extract all tokens for each paragraph and add them to the `tokens` list.\n",
        ">\n",
        "> Once you have completed your implementation, run the next two cells to build the list and test your code.\n",
        "------"
      ],
      "metadata": {
        "id": "4FlJiQ0nPscn"
      }
    },
    {
      "metadata": {
        "id": "oqGWDrXokzbj",
        "outputId": "86724bf7-7c63-45e2-ad5d-bbda78f62528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "\n",
        "for text in dataset:\n",
        "    for token in space_tokenize(text):\n",
        "        tokens.append(token)\n",
        "\n",
        "# Add your code here.\n",
        "\n",
        "print(f\"Total number of tokens in the Africa Galore dataset: {len(tokens):,}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens in the Africa Galore dataset: 19,065\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your code\n",
        "\n",
        "slm.test_build_tokens_list(tokens, space_tokenize, dataset)"
      ],
      "metadata": {
        "id": "6a30c61PJ3Oa",
        "outputId": "a7fa8824-5934-4a3e-dcea-66861d275e35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_Jt1OWe9tY6n"
      },
      "cell_type": "markdown",
      "source": [
        "To remind yourself what the tokenized dataset looks like, run the following cell to print the first 30 tokens of the first paragraph in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "Pdr8290eEum9",
        "outputId": "cbb12e8c-5e72-4920-8a89-f706fb1c5fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "tokens[:30]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Lagos',\n",
              " 'air',\n",
              " 'was',\n",
              " 'thick',\n",
              " 'with',\n",
              " 'humidity,',\n",
              " 'but',\n",
              " 'the',\n",
              " 'energy',\n",
              " 'in',\n",
              " 'the',\n",
              " 'club',\n",
              " 'was',\n",
              " 'electric.',\n",
              " 'The',\n",
              " 'band',\n",
              " 'launched',\n",
              " 'into',\n",
              " 'a',\n",
              " 'hypnotic',\n",
              " 'Afrobeat',\n",
              " 'groove,',\n",
              " 'the',\n",
              " 'drums',\n",
              " 'pounding',\n",
              " 'out',\n",
              " 'a',\n",
              " 'complex',\n",
              " 'polyrhythm,']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": 11
    },
    {
      "metadata": {
        "id": "OWRQLYO7hhp4"
      },
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2: Build the vocabulary\n",
        "\n",
        "Transformer models use a fixed set of tokens that they can process and generate. This set of tokens is known as the **vocabulary**. In many cases, this vocabulary is set to the list of unique tokens that appear in the data that the model is being trained on."
      ]
    },
    {
      "metadata": {
        "id": "FbYDEE-Hymhk"
      },
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> 💻 **Your task:**\n",
        ">\n",
        "> Complete the `build_vocabulary` function that should return the list of\n",
        "> unique tokens that appear in `tokens`.\n",
        ">>\n",
        "> Once you have implemented this function, run the two cells to define the function and test your code.\n",
        "------"
      ]
    },
    {
      "metadata": {
        "id": "ueYSAjGzj2jp"
      },
      "cell_type": "code",
      "source": [
        "def build_vocabulary(tokens: list[str]) -> list[str]:\n",
        "    # Build a vocabulary list from the set of tokens.\n",
        "    vocabulary = list(set(tokens))\n",
        "    return vocabulary"
      ],
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "q9Fk7rH5kF58",
        "cellView": "form",
        "outputId": "3eebc548-fc05-4064-edf4-b61cd755c0d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your code\n",
        "slm.test_build_vocabulary(build_vocabulary)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Nice! Your answer looks correct.\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "2RXLjfFozd9a"
      },
      "cell_type": "markdown",
      "source": [
        "You can now use the function that you have implemented to construct the vocabulary for the Africa Galore dataset.\n",
        "\n",
        "Run the cell below to create the vocabulary and print its size, that is, the number of unique tokens in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "t-mULo_viTXK",
        "outputId": "227dbe9a-0b1a-458c-e261-e787096568a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "vocabulary = build_vocabulary(tokens)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "print(\n",
        "    \"Total number of unique tokens in the Africa Galore dataset:\"\n",
        "    f\" {vocabulary_size:,}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique tokens in the Africa Galore dataset: 5,260\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "LX9mlGBEzpjz"
      },
      "cell_type": "markdown",
      "source": [
        "To get a sense of what the vocabulary looks like, run the following cell that prints the first 30 entries of the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "YQFWt9KQGN6i",
        "outputId": "0eb28e45-2327-4057-995a-cc49fd2e462f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "vocabulary[:30]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'certain',\n",
              " 'Kwame',\n",
              " 'rhinoceros',\n",
              " 'portion',\n",
              " 'quantities',\n",
              " 'Sofia,',\n",
              " 'aftershave.',\n",
              " 'experiment',\n",
              " 'greens).',\n",
              " 'create',\n",
              " 'knowledge',\n",
              " 'south,',\n",
              " 'infectious',\n",
              " 'glistening,',\n",
              " 'Kilimanjaro',\n",
              " 'donuts',\n",
              " 'building',\n",
              " 'magical',\n",
              " 'dissolve',\n",
              " 'power,',\n",
              " 'Victoria',\n",
              " '10',\n",
              " 'xima',\n",
              " 'Camel',\n",
              " 'riads',\n",
              " 'Mohammed',\n",
              " 'When',\n",
              " 'clay',\n",
              " 'coat.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        " Note that, unlike when you printed the first 30 tokens in the dataset, there are no duplicate entries. Every token appears exactly once in the vocabulary."
      ],
      "metadata": {
        "id": "1OGfWejzSLx9"
      }
    },
    {
      "metadata": {
        "id": "-Z6vtwjGLQcH"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert the tokens into token IDs (indices)\n",
        "\n",
        "As discussed above, in order to train a transformer on a text dataset, you have to turn the text data into a list of **token IDs**. These IDs are numbers such that each token maps uniquely to a different number. The IDs should always be consecutive. That means that, if the vocabulary has size $k$, then each token should map to an ID between 0 and $k-1$.\n",
        "\n",
        "In practice, the translation between tokens and IDs is implemented using two dictionaries:\n",
        "\n",
        "1. **`token_to_index`**: This dictionary maps each token in the vocabulary to its corresponding ID (index). The index must be between 0 and the vocabulary size $k-1$.\n",
        "2. **`index_to_token`**: This dictionary maps an ID (index) back to its corresponding token (a string). Given an index between 0 and $k-1$, it returns the token at that position.\n",
        "\n",
        "When you need to convert a token to a number, use `token_to_index`.\n",
        "And when you need to convert a number back to a token, use `index_to_token`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build `token_to_index`"
      ],
      "metadata": {
        "id": "Yzf768LIWZ_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell shows you how to implement the construction of the `token_to_index` mapping from the vocabulary. If you are not familiar with the [`enumerate`](https://docs.python.org/3/library/functions.html#enumerate) function in Python, print the `index` and `token` on each iteration to get a sense of what it is doing and why this code is creating the correct mapping."
      ],
      "metadata": {
        "id": "g_4HUhihVtxm"
      }
    },
    {
      "metadata": {
        "id": "mP40_BT2VWpK"
      },
      "cell_type": "code",
      "source": [
        "# Build the `token_to_index` dictionary.\n",
        "token_to_index = {}\n",
        "\n",
        "for index, token in enumerate(vocabulary):\n",
        "    token_to_index[token] = index"
      ],
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 3: Build `index_to_token`"
      ],
      "metadata": {
        "id": "0ODRXS1UVsNB"
      }
    },
    {
      "metadata": {
        "id": "jmBQMQQXVWpI"
      },
      "cell_type": "markdown",
      "source": [
        "Next, create a dictionary below called `index_to_token`, where the index is the key and the token is the value. This dictionary should be the reverse of the `token_to_index` dictionary. After implementing the dictionary, run the cell and verify that the tokens and their corresponding indexes match between `token_to_index` and `index_to_token`:"
      ]
    },
    {
      "metadata": {
        "id": "HpM5X-RNW89A"
      },
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> 💻 **Your task:**\n",
        ">\n",
        "> Complete the following cell such that it constructs the `index_to_token` mapping that maps all token IDs to their corresponding strings versions of the tokens.\n",
        ">\n",
        "> **Hint**: It may be useful to iterate through the entries in `token_to_index` using  `token_to_index.items()` to obtain the pairs of indices and tokens.\n",
        ">\n",
        "> Once you have implemented this function, run the two cells to construct the dictionary and test your code.\n",
        "------"
      ]
    },
    {
      "metadata": {
        "id": "rXlsJqD1VWpK"
      },
      "cell_type": "code",
      "source": [
        "# Create a dictionary that maps an index (a number) back to\n",
        "# its corresponding token in the vocab.\n",
        "index_to_token = {}\n",
        "\n",
        "for index, token in enumerate(vocabulary):\n",
        "    index_to_token[index] = token\n",
        "\n",
        "# Add your code here."
      ],
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your code\n",
        "slm.test_index_to_token(index_to_token, vocabulary)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CvATRQpdVWpL",
        "outputId": "12dc92bf-00f6-41f0-e6dc-7fba2b2860a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how `token_to_index` and `index_to_token` are inverse mappings, take a look at ten entries of both dictionaries:"
      ],
      "metadata": {
        "id": "d2fFi34dbkBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"token_to_index:\\n\")\n",
        "\n",
        "count = 0\n",
        "first_ten_indices = []\n",
        "for token, token_id in token_to_index.items():\n",
        "    print(f\"'{token}': {token_id}\")\n",
        "    first_ten_indices.append(token_id)\n",
        "    count += 1\n",
        "    if count == 10:\n",
        "        break\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"index_to_token:\\n\")\n",
        "for token_id in first_ten_indices:\n",
        "    print(f\"{token_id}: '{index_to_token[token_id]}'\")"
      ],
      "metadata": {
        "id": "-XLmjxpfbw4r",
        "outputId": "9b58f270-3ecc-4e6d-8847-1571053e5cd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_to_index:\n",
            "\n",
            "'': 0\n",
            "'certain': 1\n",
            "'Kwame': 2\n",
            "'rhinoceros': 3\n",
            "'portion': 4\n",
            "'quantities': 5\n",
            "'Sofia,': 6\n",
            "'aftershave.': 7\n",
            "'experiment': 8\n",
            "'greens).': 9\n",
            "\n",
            "\n",
            "\n",
            "index_to_token:\n",
            "\n",
            "0: ''\n",
            "1: 'certain'\n",
            "2: 'Kwame'\n",
            "3: 'rhinoceros'\n",
            "4: 'portion'\n",
            "5: 'quantities'\n",
            "6: 'Sofia,'\n",
            "7: 'aftershave.'\n",
            "8: 'experiment'\n",
            "9: 'greens).'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see above that the first ten tokens all have IDs between zero and ten and these IDs map back to exactly the same ten tokens."
      ],
      "metadata": {
        "id": "Sh-JnvCCcJW7"
      }
    },
    {
      "metadata": {
        "id": "VgOkb5VSwnbm"
      },
      "cell_type": "markdown",
      "source": [
        "## Encode and decode functions\n",
        "\n",
        "Rather than manually translating between lists of tokens and lists of token indices, it can be much easier to convert between these two representations of your data by implementing an `encode` and a `decode` function.\n",
        "\n",
        "- The `encode` function takes a string of text and returns the corresponding indices of the tokens.\n",
        "- The `decode` function takes a list of indices and returns the text associated with it.\n",
        "\n",
        "The following cell provides an implementation of these two functions. Run it to define both of them."
      ]
    },
    {
      "metadata": {
        "id": "JC_dnmc2xjBP"
      },
      "cell_type": "code",
      "source": [
        "def encode(text: str) -> list[int]:\n",
        "    \"\"\"Encodes a text sequence into a list of indices based on the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to be encoded.\n",
        "\n",
        "    Returns:\n",
        "        A list of indices corresponding to the tokens in the input text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert tokens into indices.\n",
        "    indices = []\n",
        "    for token in space_tokenize(text):\n",
        "        token_index = token_to_index.get(token)\n",
        "        indices.append(token_index)\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "def decode(indices: int | list[int]) -> list[str]:\n",
        "    \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
        "\n",
        "    Args:\n",
        "        indices: A single index or a list of indices to be decoded into tokens.\n",
        "\n",
        "    Returns:\n",
        "        str: A string of decoded tokens corresponding to the input indices.\n",
        "    \"\"\"\n",
        "\n",
        "    # If a single integer is passed, convert it into a list.\n",
        "    if isinstance(indices, int):\n",
        "        indices = [indices]\n",
        "\n",
        "    # Map indices to tokens.\n",
        "    tokens = []\n",
        "    for index in indices:\n",
        "        token = index_to_token.get(index)\n",
        "        tokens.append(token)\n",
        "\n",
        "    # Join the decoded tokens into a single string.\n",
        "    return \" \".join(tokens)"
      ],
      "outputs": [],
      "execution_count": 22
    },
    {
      "metadata": {
        "id": "GXnEuuHPH-Rx"
      },
      "cell_type": "markdown",
      "source": [
        "To verify that these functions are working as expected, you can encode a text so that its tokens are mapped to indices and then decode those indices. The decoding step should return the original text.\n",
        "\n",
        "The following cell prints again the first paragraph in the dataset:"
      ]
    },
    {
      "metadata": {
        "id": "84E5f-ME_0Fb",
        "outputId": "eda2c7b4-e2f1-464f-e644-bc3c49d9993b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "text = dataset[0]\n",
        "print(text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Lagos air was thick with humidity, but the energy in the club was electric. The band launched into a hypnotic Afrobeat groove, the drums pounding out a complex polyrhythm, the horns blaring a soaring melody, and the bass laying down a deep, funky foundation. A woman named Imani moved effortlessly to the music, her body swaying in time with the rhythm. The music seemed to flow through her, a powerful current of energy and joy. All around her, people were dancing, singing, and clapping, caught up in the infectious rhythm. The music was more than just entertainment; it was a celebration of life, a connection to their shared heritage, a vibrant expression of the soul of Lagos.\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "metadata": {
        "id": "7FsCmasazTgc"
      },
      "cell_type": "markdown",
      "source": [
        "Run the following cell to encode the pargraph above and look at the first ten indices."
      ]
    },
    {
      "metadata": {
        "id": "6K2wAHCKHknD",
        "outputId": "96f38504-3490-4bc0-ba6f-e013442a8bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "encode(text)[:10]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3635, 2603, 5157, 212, 316, 656, 3499, 3830, 1187, 494]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "execution_count": 23
    },
    {
      "metadata": {
        "id": "IdE_qUfezXSo"
      },
      "cell_type": "markdown",
      "source": [
        "Now decode these indices to obtain the first ten tokens of the original paragraph. This should be the same as the beginning of the original paragraph above."
      ]
    },
    {
      "metadata": {
        "id": "y65u6LNEHigD",
        "outputId": "fac2b5ed-3f4b-410f-d8f4-af15e841deed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "decode(encode(text)[:10])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Lagos air was thick with humidity, but the energy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package the methods from above in a Python class"
      ],
      "metadata": {
        "id": "grkhPodMh_z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have now implemented the most important pre-processing steps for preparing a dataset to be used for training a transformer model.\n",
        "\n",
        "In order to save you from always having to go through these steps and implement these functions whenever you want to train a model, it can be useful to define a class, e.g., `SimpleWordTokenizer`, that includes methods for extracting the vocabulary, building the `token_to_index` and `index_to_token` mappings, and implementing the `encode` and `decode` methods."
      ],
      "metadata": {
        "id": "qJyBxI2_h4e6"
      }
    },
    {
      "metadata": {
        "id": "kllG8M7e4UjH"
      },
      "cell_type": "markdown",
      "source": [
        "The `SimpleWordTokenizer` class below provides a solid foundation for understanding tokenization methods used for preparing the input for language models. As you continue to explore the world of language models further, you will come across other tokenization techniques that follow a similar structure.\n",
        "\n",
        "Make sure to go through each component of this class to remind yourself which steps are involved in constructing the vocabulary and translating between tokens and their corresponding IDs."
      ]
    },
    {
      "metadata": {
        "id": "nPk5t3kUwpOy"
      },
      "cell_type": "code",
      "source": [
        "class SimpleWordTokenizer:\n",
        "    \"\"\"A simple word tokenizer that can be initialized with a corpus of texts\n",
        "       or using a provided vocabulary list.\n",
        "\n",
        "    The tokenizer splits the text sequence based on spaces,\n",
        "    using the `encode` method to convert the text into a sequence of indices\n",
        "    and the `decode` method to convert indices back into text.\n",
        "\n",
        "    Typical usage example:\n",
        "\n",
        "        corpus = \"Hello there!\"\n",
        "        tokenizer = SimpleWordTokenizer(corpus)\n",
        "        print(tokenizer.encode('Hello'))\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus: list[str], vocabulary: list[str] | None = None):\n",
        "        \"\"\"Initializes the tokenizer with texts in corpus or with a vocabulary.\n",
        "\n",
        "        Args:\n",
        "            corpus: Input text dataset.\n",
        "            vocabulary: A pre-defined vocabulary. If None,\n",
        "                the vocabulary is automatically inferred from the texts.\n",
        "        \"\"\"\n",
        "\n",
        "        if vocabulary is None:\n",
        "            # Build the vocabulary from scratch.\n",
        "            if isinstance(corpus, str):\n",
        "                corpus = [corpus]\n",
        "\n",
        "            # Convert text sequence to tokens.\n",
        "            tokens = []\n",
        "            for text in corpus:\n",
        "                for token in self.space_tokenize(text):\n",
        "                    tokens.append(token)\n",
        "\n",
        "            # Create a vocabulary comprising of unique tokens.\n",
        "            self.vocabulary = self.build_vocabulary(tokens)\n",
        "\n",
        "        else:\n",
        "            self.vocabulary = vocabulary\n",
        "\n",
        "        # Size of vocabulary.\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "\n",
        "        # Create token-to-index and index-to-token mappings.\n",
        "        self.token_to_index = {}\n",
        "        self.index_to_token = {}\n",
        "        # Loop through all tokens in the vocabulary. enumerate automatically\n",
        "        # assigns a unique index to each token.\n",
        "        for index, token in enumerate(self.vocabulary):\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "\n",
        "    def space_tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given text on space into tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Text to split on space.\n",
        "\n",
        "        Returns:\n",
        "            List of tokens after splitting `text`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use re.split such that multiple spaces are treated as a single\n",
        "        # separator.\n",
        "        return re.split(\" +\", text)\n",
        "\n",
        "    def join_text(self, text_list: list[str]) -> str:\n",
        "        \"\"\"Combines a list of tokens into a single string, with tokens separated\n",
        "           by spaces.\n",
        "\n",
        "        Args:\n",
        "            text_list: List of tokens to be joined.\n",
        "\n",
        "        Returns:\n",
        "            String with all tokens joined with a space.\n",
        "\n",
        "        \"\"\"\n",
        "        return \" \".join(text_list)\n",
        "\n",
        "    def build_vocabulary(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Create a vocabulary list from the list of tokens.\n",
        "\n",
        "        Args:\n",
        "            tokens: The list of tokens in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of unique tokens (vocabulary) in the dataset.\n",
        "        \"\"\"\n",
        "        return sorted(list(set(tokens)))\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Encodes a text sequence into a list of indices.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to be encoded.\n",
        "\n",
        "        Returns:\n",
        "            A list of indices corresponding to the tokens in the input text.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert tokens into indices.\n",
        "        indices = []\n",
        "        for token in self.space_tokenize(text):\n",
        "            token_index = self.token_to_index.get(token)\n",
        "            indices.append(token_index)\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices: int | list[int]) -> str:\n",
        "        \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
        "\n",
        "        Args:\n",
        "            indices: A single index or a list of indices to be decoded into\n",
        "                tokens.\n",
        "\n",
        "        Returns:\n",
        "            str: A string of decoded tokens corresponding to the input indices.\n",
        "        \"\"\"\n",
        "\n",
        "        # If a single integer is passed, convert it into a list.\n",
        "        if isinstance(indices, int):\n",
        "            indices = [indices]\n",
        "\n",
        "        # Map indices to tokens.\n",
        "        tokens = []\n",
        "        for index in indices:\n",
        "            token = self.index_to_token.get(index)\n",
        "            tokens.append(token)\n",
        "\n",
        "        # Join the decoded tokens into a single string.\n",
        "        return self.join_text(tokens)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "mgV0fzQB5LrN"
      },
      "cell_type": "markdown",
      "source": [
        "To observe that this class performs the same processing as your previous implementations, run the following cell. This cell runs some tests that verify that the first paragraph from the dataset remains the same after encoding and then decoding it using the `SimpleWordTokenizer`."
      ]
    },
    {
      "metadata": {
        "id": "0rMKmNlRY-3q"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleWordTokenizer(dataset)\n",
        "slm.test_simple_word_tokenizer(tokenizer, vocabulary, dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "sZdmYZ0Bqfn9"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This is the end of the **Prepare The Dataset For Training a SLM** lab.\n",
        "\n",
        "This lab guided you through the steps necessary for preparing a text dataset to be used for training a small language model (SLM). You focused on:\n",
        "\n",
        "- **Loading and exploring the dataset:** You examined the structure and content of the Africa Galore dataset and inspected example paragraphs in the dataset.\n",
        "\n",
        "- **Tokenizing the text:** You used a simple word-level tokenization method to split the text into individual tokens and created a vocabulary of unique tokens.\n",
        "\n",
        "- **Creating numerical representations:** You mapped each token to a unique numerical index by creating `token_to_index` and `index_to_token` dictionaries, which enable the conversion between tokens and token IDs.\n",
        "\n",
        "- **Packaging the steps in a tokenizer class:** You examined a consolidated version of the tokenization and encoding/decoding logic in a reusable `SimpleWordTokenizer` class. This class streamlines the process of converting text into numerical data that can be fed into a language model and converting the output of a language model to human-readable texts.\n",
        "\n",
        "In the next lab, you will use this tokenizer class to tokenize the data and use this data to train a small language model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities above. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "However, we recommend that you *only* look at the solutions after you have tried to solve the activities above *multiple times*. The best way to learn challenging concepts in computer science and artifical intelligence is to debug your code piece-by-piece until it works rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code, for example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also make you practice how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them and then type them manually into the cell. This will help you understand where you went wrong."
      ],
      "metadata": {
        "id": "W9eSg4k_QE9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1"
      ],
      "metadata": {
        "id": "kWRihzWaQGnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this code to the cell in the coding activity above to build the list of\n",
        "# tokens.\n",
        "for paragraph in dataset:\n",
        "    for token in space_tokenize(paragraph):\n",
        "        tokens.append(token)"
      ],
      "metadata": {
        "id": "GOuSPwbNQKtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2"
      ],
      "metadata": {
        "id": "9gQPoK4TkH1q"
      }
    },
    {
      "metadata": {
        "id": "ZeJlbyiBjla4"
      },
      "cell_type": "code",
      "source": [
        "# Complete implementation of the build_vocab function.\n",
        "def build_vocabulary(tokens: list[str])-> list[str]:\n",
        "\n",
        "    # Build a vocabulary list from the set of tokens.\n",
        "    vocab = list(set(tokens))\n",
        "    return vocab"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 3\n"
      ],
      "metadata": {
        "id": "_dyB98kFkTF9"
      }
    },
    {
      "metadata": {
        "id": "Xan-_ZYxtUpk"
      },
      "cell_type": "code",
      "source": [
        "# Add this code to the cell in the coding activity above to build the\n",
        "# index_to_token mapping.\n",
        "for token, index in token_to_index.items():\n",
        "    index_to_token[index] = token"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "W9eSg4k_QE9E",
        "kWRihzWaQGnE",
        "9gQPoK4TkH1q",
        "_dyB98kFkTF9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}